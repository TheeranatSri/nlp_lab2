{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "##Reference\n",
        "##https://huggingface.co/docs/transformers/quicktour"
      ],
      "metadata": {
        "id": "PP7AuzRTiekD"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hoI0lNAacC-Y"
      },
      "outputs": [],
      "source": [
        "# Transformers installation\n",
        "! pip install transformers datasets\n",
        "# To install from source instead of the last release, comment the command above and uncomment the following one.\n",
        "# ! pip install git+https://github.com/huggingface/transformers.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qolexx7OcC-c"
      },
      "source": [
        "# Quick tour"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4NKANxnecC-d"
      },
      "source": [
        "Get up and running with ü§ó Transformers! Whether you're a developer or an everyday user, this quick tour will help you get started and show you how to use the [pipeline()](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline) for inference, load a pretrained model and preprocessor with an [AutoClass](https://huggingface.co/docs/transformers/main/en/./model_doc/auto), and quickly train a model with PyTorch or TensorFlow. If you're a beginner, we recommend checking out our tutorials or [course](https://huggingface.co/course/chapter1/1) next for more in-depth explanations of the concepts introduced here.\n",
        "\n",
        "Before you begin, make sure you have all the necessary libraries installed:\n",
        "\n",
        "```bash\n",
        "!pip install transformers datasets\n",
        "```\n",
        "\n",
        "You'll also need to install your preferred machine learning framework:\n",
        "\n",
        "```bash\n",
        "pip install tensorflow\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LL2_WpLbcC-e"
      },
      "source": [
        "## Pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "hide_input": true,
        "id": "YFcvxGfQcC-f"
      },
      "outputs": [],
      "source": [
        "#@title\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/tiZFewofSLM?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hXIA0k2McC-f"
      },
      "source": [
        "The [pipeline()](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline) is the easiest and fastest way to use a pretrained model for inference. You can use the [pipeline()](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline) out-of-the-box for many tasks across different modalities, some of which are shown in the table below:\n",
        "\n",
        "<Tip>\n",
        "\n",
        "For a complete list of available tasks, check out the [pipeline API reference](https://huggingface.co/docs/transformers/main/en/./main_classes/pipelines).\n",
        "\n",
        "</Tip>\n",
        "\n",
        "| **Task**                     | **Description**                                                                                              | **Modality**    | **Pipeline identifier**                       |\n",
        "|------------------------------|--------------------------------------------------------------------------------------------------------------|-----------------|-----------------------------------------------|\n",
        "| Text classification          | assign a label to a given sequence of text                                                                   | NLP             | pipeline(task=‚Äúsentiment-analysis‚Äù)           |\n",
        "| Text generation              | generate text given a prompt                                                                                 | NLP             | pipeline(task=‚Äútext-generation‚Äù)              |\n",
        "| Summarization                | generate a summary of a sequence of text or document                                                         | NLP             | pipeline(task=‚Äúsummarization‚Äù)                |\n",
        "| Image classification         | assign a label to an image                                                                                   | Computer vision | pipeline(task=‚Äúimage-classification‚Äù)         |\n",
        "| Image segmentation           | assign a label to each individual pixel of an image (supports semantic, panoptic, and instance segmentation) | Computer vision | pipeline(task=‚Äúimage-segmentation‚Äù)           |\n",
        "| Object detection             | predict the bounding boxes and classes of objects in an image                                                | Computer vision | pipeline(task=‚Äúobject-detection‚Äù)             |\n",
        "| Audio classification         | assign a label to some audio data                                                                            | Audio           | pipeline(task=‚Äúaudio-classification‚Äù)         |\n",
        "| Automatic speech recognition | transcribe speech into text                                                                                  | Audio           | pipeline(task=‚Äúautomatic-speech-recognition‚Äù) |\n",
        "| Visual question answering    | answer a question about the image, given an image and a question                                             | Multimodal      | pipeline(task=‚Äúvqa‚Äù)                          |\n",
        "| Document question answering  | answer a question about a document, given an image and a question                                            | Multimodal      | pipeline(task=\"document-question-answering\")  |\n",
        "| Image captioning             | generate a caption for a given image                                                                         | Multimodal      | pipeline(task=\"image-to-text\")                |\n",
        "\n",
        "Start by creating an instance of [pipeline()](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline) and specifying a task you want to use it for. In this guide, you'll use the [pipeline()](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline) for sentiment analysis as an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzC9AR3ZcC-h"
      },
      "outputs": [],
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "classifier = pipeline(\"sentiment-analysis\") #classifier = pipeline(task=\"sentiment-analysis\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGVvHQ1NcC-i"
      },
      "source": [
        "The [pipeline()](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline) downloads and caches a default [pretrained model](https://huggingface.co/distilbert-base-uncased-finetuned-sst-2-english) and tokenizer for sentiment analysis. Now you can use the `classifier` on your target text:\n",
        "You can see the second sentence has been classified as negative (it needs to be positive or negative) but its score is fairly neutral.\n",
        "\n",
        "By default, the model downloaded for this pipeline is called \"distilbert-base-uncased-finetuned-sst-2-english\". We can look at its model page to get more information about it. It uses the [DistilBERT architecture](https://huggingface.co/docs/transformers/model_doc/distilbert) and has been fine-tuned on a dataset called [SST-2](https://huggingface.co/datasets/sst2) for the sentiment analysis task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5RnlVcAcC-i"
      },
      "outputs": [],
      "source": [
        "#one sentence\n",
        "classifier(\"We are very happy to show you the ü§ó Transformers library.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#two sentences\n",
        "results = classifier([\"I love you\",\"I hate you.\"])\n",
        "results"
      ],
      "metadata": {
        "id": "91daFzypmiCh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Label : {results[0]['label']}\")\n",
        "print(f\"Label : {results[1]['label']}\")"
      ],
      "metadata": {
        "id": "nYqxqP9RwZcp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXtUj-KAcC-k"
      },
      "source": [
        "If you have more than one input, pass your inputs as a list to the [pipeline()](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline) to return a list of dictionaries:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QkNrKFVGcC-l"
      },
      "outputs": [],
      "source": [
        "#list of sentences\n",
        "results = classifier([\"We are very happy to show you the ü§ó Transformers library.\", \"We hope you don't hate it.\"])\n",
        "for result in results:\n",
        "    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kRKV_UEPcC-n"
      },
      "source": [
        "For larger datasets where the inputs are big (like in speech or vision), you'll want to pass a generator instead of a list to load all the inputs in memory. Take a look at the [pipeline API reference](https://huggingface.co/docs/transformers/main/en/./main_classes/pipelines) for more information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cZ6JY2UlcC-n"
      },
      "source": [
        "### Use another model and tokenizer in the pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Th0BcNiUcC-n"
      },
      "source": [
        "The [pipeline()](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline) can accommodate any model from the [Hub](https://huggingface.co/models), making it easy to adapt the [pipeline()](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline) for other use-cases. For example, if you'd like a model capable of handling French text, use the tags on the Hub to filter for an appropriate model. The top filtered result returns a multilingual [BERT model](https://huggingface.co/nlptown/bert-base-multilingual-uncased-sentiment) finetuned for sentiment analysis you can use for French text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e6DlB9RzcC-o"
      },
      "outputs": [],
      "source": [
        "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9C23BtMcC-o"
      },
      "source": [
        "Use [TFAutoModelForSequenceClassification](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification) and [AutoTokenizer](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer) to load the pretrained model and it's associated tokenizer (more on an `TFAutoClass` in the next section):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZsTXkhzQcC-o"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, TFAutoModelForSequenceClassification\n",
        "\n",
        "model = TFAutoModelForSequenceClassification.from_pretrained(model_name)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZZppzXLUcC-o"
      },
      "outputs": [],
      "source": [
        "classifier = pipeline(task = \"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
        "classifier(\"We are very happy to introduce you to the library ü§ó Transformers\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1EC4sRR-cC-o"
      },
      "source": [
        "Specify the model and tokenizer in the [pipeline()](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline), and now you can apply the `classifier` on French text:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xsS9cuQ6w-JY"
      },
      "outputs": [],
      "source": [
        "#French\n",
        "classifier = pipeline(task = \"sentiment-analysis\", model=model, tokenizer=tokenizer)\n",
        "classifier(\"Nous sommes tr√®s heureux de vous pr√©senter la biblioth√®que ü§ó Transformers.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Try with another model of text classification (multiclass)"
      ],
      "metadata": {
        "id": "xzVrLqWeyYxb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Mpnq1W3Jyjdw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PPGgISFXR_-F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Try with another task of text processing"
      ],
      "metadata": {
        "id": "nv_onrfvyvMI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Text generation"
      ],
      "metadata": {
        "id": "XMLdhyTR0aZI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " "
      ],
      "metadata": {
        "id": "81F0wmIgy5ro"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " "
      ],
      "metadata": {
        "id": "-x_zG2tcy5zm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " "
      ],
      "metadata": {
        "id": "Xdntyt_Ez_RR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Summarization"
      ],
      "metadata": {
        "id": "U1W2rtqK0cVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        " "
      ],
      "metadata": {
        "id": "S2UbH5a9yjSY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        " "
      ],
      "metadata": {
        "id": "_iizcvGL4EoS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JA3OAER1cC-o"
      },
      "source": [
        "If you can't find a model for your use-case, you'll need to finetune a pretrained model on your data. Take a look at our [finetuning tutorial](https://huggingface.co/docs/transformers/main/en/./training) to learn how. Finally, after you've finetuned your pretrained model, please consider [sharing](https://huggingface.co/docs/transformers/main/en/./model_sharing) the model with the community on the Hub to democratize machine learning for everyone! ü§ó"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jRqlKcPncC-p"
      },
      "source": [
        "## AutoClass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "hide_input": true,
        "id": "2y-amz6icC-p",
        "outputId": "175692da-e78d-4949-d837-985f427b8866",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/IPython/core/display.py:701: UserWarning: Consider using IPython.display.IFrame instead\n",
            "  warnings.warn(\"Consider using IPython.display.IFrame instead\")\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/AhChOFRegn4?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "#@title\n",
        "from IPython.display import HTML\n",
        "\n",
        "HTML('<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/AhChOFRegn4?rel=0&amp;controls=0&amp;showinfo=0\" frameborder=\"0\" allowfullscreen></iframe>')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9n2tbXSacC-p"
      },
      "source": [
        "Under the hood, the [AutoModelForSequenceClassification](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModelForSequenceClassification) and [AutoTokenizer](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer) classes work together to power the [pipeline()](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline) you used above. An [AutoClass](https://huggingface.co/docs/transformers/main/en/./model_doc/auto) is a shortcut that automatically retrieves the architecture of a pretrained model from its name or path. You only need to select the appropriate `AutoClass` for your task and it's associated preprocessing class. \n",
        "\n",
        "Let's return to the example from the previous section and see how you can use the `AutoClass` to replicate the results of the [pipeline()](https://huggingface.co/docs/transformers/main/en/main_classes/pipelines#transformers.pipeline)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cuDumHqGcC-p"
      },
      "source": [
        "### AutoTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1BGPzEZHcC-p"
      },
      "source": [
        "A tokenizer is responsible for preprocessing text into an array of numbers as inputs to a model. There are multiple rules that govern the tokenization process, including how to split a word and at what level words should be split (learn more about tokenization in the [tokenizer summary](https://huggingface.co/docs/transformers/main/en/./tokenizer_summary)). The most important thing to remember is you need to instantiate a tokenizer with the same model name to ensure you're using the same tokenization rules a model was pretrained with.\n",
        "\n",
        "Load a tokenizer with [AutoTokenizer](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer):"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Example of BertTokenizer"
      ],
      "metadata": {
        "id": "BnKPO9at_sVX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Simple AutoTokenizer\n",
        "from transformers import BertTokenizer\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "tokenizer.tokenize(\"I have a new GPU!\")"
      ],
      "metadata": {
        "id": "0Sd2PsMk_Xqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer.tokenize(\"We are very happy to show you the ü§ó Transformers library.\")"
      ],
      "metadata": {
        "id": "et-BH5Yw_hVZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text =[\"We are very happy to show you the ü§ó Transformers library.\"]\n",
        "tokenizer.batch_encode_plus(text, padding=True, return_token_type_ids=False)"
      ],
      "metadata": {
        "id": "8SxxLtClnmJU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "ONkwmfFconHi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LWVz8HvScC-q"
      },
      "outputs": [],
      "source": [
        "#AutoTokenizer\n",
        "from transformers import AutoTokenizer\n",
        "\n",
        "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZDICoRhcC-q"
      },
      "source": [
        "Pass your text to the tokenizer:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PFan8qDhcC-q"
      },
      "outputs": [],
      "source": [
        "encoding = tokenizer(\"We are very happy to show you the ü§ó Transformers library.\")\n",
        "print(encoding)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDZ_Ym5fcC-q"
      },
      "source": [
        "The tokenizer returns a dictionary containing:\n",
        "\n",
        "* [input_ids](https://huggingface.co/docs/transformers/main/en/./glossary#input-ids): numerical representations of your tokens.\n",
        "* [attention_mask](https://huggingface.co/docs/transformers/main/en/.glossary#attention-mask): indicates which tokens should be attended to.\n",
        "\n",
        "A tokenizer can also accept a list of inputs, and pad and truncate the text to return a batch with uniform length:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mKu4ahRrcC-q"
      },
      "outputs": [],
      "source": [
        "tf_batch = tokenizer(\n",
        "    [\"We are very happy to show you the ü§ó Transformers library.\", \"We hope you don't hate it.\"],\n",
        "    padding=True,\n",
        "    truncation=True,\n",
        "    max_length=512,\n",
        "    return_tensors=\"tf\",\n",
        ")\n",
        "tf_batch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AHicqg8ZcC-q"
      },
      "source": [
        "<Tip>\n",
        "\n",
        "Check out the [preprocess](https://huggingface.co/docs/transformers/main/en/./preprocessing) tutorial for more details about tokenization, and how to use an [AutoImageProcessor](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoImageProcessor), [AutoFeatureExtractor](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoFeatureExtractor) and [AutoProcessor](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoProcessor) to preprocess image, audio, and multimodal inputs.\n",
        "\n",
        "</Tip>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVEQhA8scC-q"
      },
      "source": [
        "### AutoModel"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GD-LEaA4cC-r"
      },
      "source": [
        "ü§ó Transformers provides a simple and unified way to load pretrained instances. This means you can load an [TFAutoModel](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.TFAutoModel) like you would load an [AutoTokenizer](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoTokenizer). The only difference is selecting the correct [TFAutoModel](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.TFAutoModel) for the task. For text (or sequence) classification, you should load [TFAutoModelForSequenceClassification](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.TFAutoModelForSequenceClassification):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ktFqT8d2cC-r"
      },
      "outputs": [],
      "source": [
        "from transformers import TFAutoModelForSequenceClassification\n",
        "\n",
        "model_name = \"nlptown/bert-base-multilingual-uncased-sentiment\"\n",
        "tf_model = TFAutoModelForSequenceClassification.from_pretrained(model_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMmaQd1hcC-r"
      },
      "source": [
        "<Tip>\n",
        "\n",
        "See the [task summary](https://huggingface.co/docs/transformers/main/en/./task_summary) for tasks supported by an [AutoModel](https://huggingface.co/docs/transformers/main/en/model_doc/auto#transformers.AutoModel) class.\n",
        "\n",
        "</Tip>\n",
        "\n",
        "Now pass your preprocessed batch of inputs directly to the model by passing the dictionary keys directly to the tensors:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xouweuArcC-r"
      },
      "outputs": [],
      "source": [
        "tf_outputs = tf_model(tf_batch)\n",
        "print(tf_outputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aNo1UbmcC-r"
      },
      "source": [
        "The model outputs the final activations in the `logits` attribute. Apply the softmax function to the `logits` to retrieve the probabilities:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8izxt6picC-r"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "tf_predictions = tf.nn.softmax(tf_outputs.logits, axis=-1)\n",
        "tf_predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7WMh-efXcC-r"
      },
      "source": [
        "### Save a model"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once your model is fine-tuned, you can save it with its tokenizer using [TFPreTrainedModel.save_pretrained()](https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel.save_pretrained):"
      ],
      "metadata": {
        "id": "mfipYHC0G_-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf_save_directory = \"./tf_save_pretrained\"\n",
        "tokenizer.save_pretrained(tf_save_directory)\n",
        "tf_model.save_pretrained(tf_save_directory)"
      ],
      "metadata": {
        "id": "FjGlwqD-GhtT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "When you are ready to use the model again, reload it with [TFPreTrainedModel.from_pretrained()](https://huggingface.co/docs/transformers/main/en/main_classes/model#transformers.TFPreTrainedModel.from_pretrained):"
      ],
      "metadata": {
        "id": "0ZHwaDjDHFdW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tf_model = TFAutoModelForSequenceClassification.from_pretrained(\"./tf_save_pretrained\")"
      ],
      "metadata": {
        "id": "INqK90m6GuXO"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "language_info": {
      "name": "python"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "gpuClass": "standard"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}